---
title: Sessions
description: Multi-turn conversations with persistent message history
---

Sessions provide a high-level API for multi-turn conversations. Messages persist across `send()`/`stream()` cycles, so the model sees the full conversation history on every turn.

## Creating a Session

```ts title="session.ts"
import { createSession } from "stratus/core";

const session = createSession({
  model,
  instructions: "You are a weather assistant.",
  tools: [getWeather],
});
```

## Send and Stream

The session API follows a simple two-step pattern:

<Steps>
<Step>

### Queue a message

`send(message)` queues a user message synchronously - no API call is made.

```ts
session.send("What's the weather in NYC?");
```

</Step>
<Step>

### Stream the response

`stream()` runs the agent loop, yielding streaming events.

```ts
for await (const event of session.stream()) {
  if (event.type === "content_delta") {
    process.stdout.write(event.content);
  }
}
```

</Step>
</Steps>

## Multi-Turn

Just call `send()` and `stream()` again. Previous messages are automatically included:

```ts title="multi-turn.ts"
session.send("What's the weather in NYC?");
for await (const event of session.stream()) {
  if (event.type === "content_delta") process.stdout.write(event.content);
}

// The model sees the full conversation so far
session.send("What about London?"); // [!code highlight]
for await (const event of session.stream()) {
  if (event.type === "content_delta") process.stdout.write(event.content);
}
```

## Multimodal Messages

`send()` accepts either a string or a `ContentPart[]` array for multimodal input:

```ts title="multimodal.ts"
import type { ContentPart } from "stratus/core";

const parts: ContentPart[] = [
  { type: "text", text: "What is in this image?" },
  { type: "image_url", image_url: { url: "https://example.com/photo.png" } },
];

session.send(parts); // [!code highlight]
for await (const event of session.stream()) {
  if (event.type === "content_delta") process.stdout.write(event.content);
}
```

<Callout type="info" title="Content Part Types">
`TextContentPart` - `{ type: "text", text: string }`

`ImageContentPart` - `{ type: "image_url", image_url: { url: string, detail?: "auto" | "low" | "high" } }`
</Callout>

The `prompt()` function also accepts `ContentPart[]`:

```ts
const result = await prompt(parts, { model });
```

## Accessing Results

After consuming the stream, access the result via `session.result`:

```ts
session.send("Summarize our conversation.");
for await (const event of session.stream()) { /* ... */ }

const result = await session.result;
console.log(result.output);        // Raw string output
console.log(result.finishReason);   // "stop", "tool_calls", etc.
console.log(result.usage);          // Token usage across this turn
console.log(result.lastAgent);      // Agent that handled this turn
```

## Message History

Access the accumulated conversation history at any time:

```ts
const messages = session.messages;
// Returns a copy - mutating it won't affect the session
```

<Callout>
Messages include all user, assistant, and tool messages from previous turns. System messages are managed internally and not included.
</Callout>

## Save, Resume, and Fork

Sessions can be saved to a snapshot and resumed or forked later. This enables persistence, branching conversations, and recovery from failures.

<Tabs items={["Save", "Resume", "Fork"]}>
<Tab value="Save">

```ts title="save.ts"
const snapshot = session.save();
// snapshot.id - same as session.id
// snapshot.messages - deep copy of the conversation history
```

<Callout type="warn">
`save()` throws if the session is closed or currently streaming.
</Callout>

</Tab>
<Tab value="Resume">

Resume a session with the same ID and conversation history:

```ts title="resume.ts"
import { resumeSession } from "stratus/core";

const session2 = resumeSession(snapshot, {
  model,
  instructions: "You are a helpful assistant.",
});

// session2.id === snapshot.id
session2.send("Continue where we left off.");
for await (const event of session2.stream()) { /* ... */ }
```

</Tab>
<Tab value="Fork">

Fork creates a new session (new ID) with a copy of the conversation history:

```ts title="fork.ts"
import { forkSession } from "stratus/core";

const forked = forkSession(snapshot, {
  model,
  instructions: "You are a helpful assistant.",
});

// forked.id !== snapshot.id
forked.send("Take a different direction.");
for await (const event of forked.stream()) { /* ... */ }
```

</Tab>
</Tabs>

## Abort Signal

Pass an `AbortSignal` to `stream()` to cancel a running turn:

```ts title="abort.ts"
import { RunAbortedError } from "stratus/core";

const ac = new AbortController();

session.send("Write a very long essay.");
try {
  for await (const event of session.stream({ signal: ac.signal })) { // [!code highlight]
    if (event.type === "content_delta") process.stdout.write(event.content);
  }
} catch (error) {
  if (error instanceof RunAbortedError) {
    console.log("Stream was cancelled");
  }
}
```

<Callout type="info">
The signal is per-invocation, not per-session. See [Streaming - Abort Signal](/docs/streaming#abort-signal) for more details.
</Callout>

## Cleanup

Sessions support both explicit cleanup and `await using`:

<Tabs items={["await using", "Explicit"]}>
<Tab value="await using">

```ts
await using session = createSession({ model });
// session.close() is called automatically when the block exits
```

</Tab>
<Tab value="Explicit">

```ts
session.close();
// After closing, send(), stream(), and save() will throw
```

</Tab>
</Tabs>

## One-Shot with `prompt()`

For single-turn use cases, `prompt()` is a convenience that creates a session, sends a message, drains the stream, and returns the result:

```ts
import { prompt } from "stratus/core";

const result = await prompt("What is 2 + 2?", { model });
console.log(result.output); // "4"
```

## Session Config

`SessionConfig` accepts the same options as `AgentConfig` (except `name`), plus `context` and `maxTurns`:

| Property | Type | Description |
| --- | --- | --- |
| `model` | `Model` | **Required.** The LLM model |
| `instructions` | `Instructions` | System prompt |
| `tools` | `FunctionTool[]` | Available tools |
| `subagents` | `SubAgent[]` | [Sub-agents](/docs/subagents) that run as tool calls |
| `modelSettings` | `ModelSettings` | Temperature, max tokens, etc. |
| `outputType` | `z.ZodType` | Structured output schema |
| `handoffs` | `HandoffInput[]` | Handoff targets |
| `inputGuardrails` | `InputGuardrail[]` | Input guardrails |
| `outputGuardrails` | `OutputGuardrail[]` | Output guardrails |
| `hooks` | `AgentHooks` | Lifecycle hooks |
| `toolUseBehavior` | `ToolUseBehavior` | Post-tool-call behavior |
| `context` | `TContext` | Shared context object |
| `maxTurns` | `number` | Max model calls per `stream()` (default: 10) |
